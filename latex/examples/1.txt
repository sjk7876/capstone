RIT Computer Science • Capstone Report • 2245

Emotion Centric Music Recommendation
Chinmay Deepak Bhate

Prof. Sean Strout

Department of Computer Science
Golisano College of Computing and Information Sciences
Rochester Institute of Technology
Rochester, NY 14586
cb4490@cs.rit.edu

Department of Computer Science
Golisano College of Computing and Information Sciences
Rochester Institute of Technology
Rochester, NY 14586
ssvcs@rit.edu

Abstract—This research introduces an integrated emotioncentric music recommendation system designed to enhance user
experience by aligning music suggestions with users’ real-time
emotional states. Music significantly influences human emotions
and behaviors, yet traditional recommendation systems often
overlook users’ dynamic emotional conditions, leading to suboptimal experiences. Leveraging a deep learning-based computer
vision module, this system performs real-time facial expression
analysis to detect user emotions. Music recommendations are
generated through a unified model that combines a supervised Multi-output Random Forest classifier for mood and era
prediction with an unsupervised K-Means clustering and KNearest Neighbors (KNN) pipeline for genre classification. The
recommendation process utilizes audio features including valence,
tempo, energy, and danceability, along with personalized user
genre preferences, to ensure highly relevant and emotionally
aligned song selection. The system’s performance was evaluated
through user surveys, achieving approximately 91% accuracy
in facial emotion recognition and around 88% accuracy in
aligning recommendations with user-reported emotional states.
User feedback indicated high overall satisfaction, demonstrating
the potential of emotion-aware methodologies to significantly
improve the music streaming experience.
Index Terms—Emotion recognition, music recommendation,
facial expression analysis, machine learning, deep learning, KMeans, Random Forest

fluctuating nature of human emotions further complicates
the task of mood prediction. To address these complexities,
this research combines facial expression analysis with audiobased features, taking advantage of recent progress in deep
learning to enable real-time emotion detection and improve
recommendation accuracy.
Previous research on emotion-aware recommendations has
employed machine learning models like K-Means clustering
and Random Forest classifiers to classify music moods from
audio features [2]. Recent advancements have integrated deep
learning techniques for facial expression-based mood detection, allowing real-time recommendation adaptation [3]. While
past systems primarily focused on audio features alone, this
research integrates facial expression analysis with musical
features and incorporates users’ genre preferences, providing
more personalized and emotionally relevant music recommendations.
This research introduces an integrated emotion-aware system that utilizes facial cues in real time to deliver personalized
music recommendations. By employing a deep learning-based
computer vision module, the system accurately detects user
emotions through facial cues, enabling music recommendations to dynamically match the listener’s current emotional
state. Leveraging visual information enhances mood detection
accuracy and significantly improves the alignment of recommendations with user emotions.
Specifically, facial emotion recognition utilizes the
lightweight MobileNetV2 convolutional neural network
architecture, optimized for real-time performance and
achieving approximately 91% accuracy on test data. The
music recommendation component integrates a supervised
Multi-output Random Forest classifier for predicting the mood
and era of music tracks, alongside an unsupervised K-Means
clustering combined with K-Nearest Neighbors (KNN)
for effective genre classification. These methodologies are
seamlessly combined to provide recommendations filtered by
the user’s detected mood and personalized genre preferences,
resulting in more relevant and emotionally resonant music
selections.
The effectiveness of the proposed system is evaluated
through comprehensive user surveys assessing metrics such as
emotion recognition accuracy, recommendation relevance, and
overall user satisfaction. User feedback indicates high satisfac-

I. I NTRODUCTION
Music plays a fundamental role in human life, serving as
a medium for emotional expression, relaxation, motivation,
and therapy. The rise of digital streaming platforms has transformed music engagement, making personalized recommendations essential. Traditional recommendation systems typically employ collaborative filtering or content-based filtering
methods, analyzing past interactions to predict preferences
[1]. While effective, these approaches fail to consider the
listener’s real-time emotional state, potentially resulting in
dissatisfaction.
Developing an emotion-centric recommendation system requires careful consideration of how music evokes emotional
responses, which are highly subjective and context-dependent.
While features like valence, tempo, and energy are useful
starting points, they often lack sufficient precision for accurately capturing the nuances of emotional reactions. Additionally, listeners’ genre preferences tend to shift depending on
their emotional states, highlighting the necessity for adaptable recommendation strategies. The inherently subjective and

Rochester Institute of Technology

1|Page

RIT Computer Science • Capstone Report • 2245

tion levels, with emotion detection achieving 88% accuracy
as per survey data, and music recommendations consistently
rated highly in terms of relevance and enjoyment. Insights
from these evaluations underscore the value of integrating realtime emotion detection into music recommendation systems,
significantly contributing toward creating emotionally intelligent and personalized music streaming experiences.
The remainder of the paper is structured as follows: The
next section details the proposed approach, including dataset
collection, preprocessing, model implementation, and system
components. Following this, the evaluation section presents
performance metrics and results. The paper concludes with
a summary of findings and suggestions for future research
directions.
II. BACKGROUND

Fig. 1. Correlation matrix of music audio feature

Recent research has pointed out that traditional music
recommendation systems relying primarily on collaborative
or content-based filtering often fail to account for listeners’
immediate emotional contexts. This limitation can lead to
recommendations that feel disconnected or irrelevant to users’
current emotional states. Although earlier audio-based methods have made important progress, they typically struggle with
accurately reflecting short-term emotional variations and personal differences among listeners. To address these shortcomings, newer multimodal techniques especially those employing
convolutional neural networks (CNNs) for facial expression
analysis have shown promising results in achieving real-time,
accurate mood detection. Despite these advancements, current systems often overlook users’ shifting genre preferences
tied explicitly to specific emotional conditions. This research
addresses these limitations by integrating advanced facial
recognition with robust machine learning approaches, thereby
offering recommendations that dynamically align with both
emotional states and user-specific genre preferences.
III. A PPROACH
The emotion-centric recommendation system integrates facial expression-based mood detection, user profiling that captures genre preferences across different moods, and personalized music recommendations to deliver highly relevant results. Upon registration, users complete a brief questionnaire
identifying their genre preferences according to different emotional states, such as happiness, sadness, or calmness. This
initial profiling enables the system to understand and predict
user preferences more accurately. Recommendations are then
dynamically tailored by combining real-time mood detection
through facial expressions with the user’s pre-defined genre
choices. For example, suppose the system detects that a user
is currently happy and the user’s profile indicates a preference
for upbeat pop and dance tracks during such moods. In that
case, the system immediately suggests songs that closely
match these criteria, enhancing emotional alignment and user
satisfaction.

A. Datasets and Preprocessing
The FER-2013 dataset [4], consisting of 35,887 labeled
facial images categorized into seven emotions (happiness,
sadness, anger, fear, surprise, disgust, and neutrality), was
utilized to train and fine-tune a MobileNetV2-based CNN
model optimized for real-time facial emotion classification.
Out of these seven available emotions, three specific categories—Happy, Sad, and Neutral—were selected and used for
model training to align closely with the intended application
in music recommendation. For music analysis, the Spotify
dataset, featuring comprehensive musical attributes such as
valence, tempo, energy, and danceability along with genre
details, was used for clustering, tagging, and classification purposes [5]. Additionally, the Spotify Moods dataset, specifically
labeled for moods, was utilized for training the Random Forest
classifier to perform supervised mood [6]. Data preprocessing
steps included normalization, handling of missing values, feature extraction, feature selection and encoding of categorical
variables to prepare datasets effectively for model training.
After thorough data cleaning, a detailed exploratory analysis
was carried out on the Spotify dataset. This involved examining correlations among audio features, studying the distribution patterns of these features, and exploring trends related
to music popularity. Figure 1 illustrates the correlation matrix
of these audio features, highlighting meaningful relationships
between attributes. For instance, valence showed a strong positive relationship with energy and an inverse relationship with
acousticness, suggesting upbeat and energetic tracks generally
have higher valence, indicating positivity, while more acoustic
tracks tend to be calmer and less energetic. Additionally,
energetic and danceable songs showed positive associations
with popularity, implying listeners’ preference for dynamic
music. Temporal analyses of the valence attribute revealed
a noticeable trend of decreasing positivity in music over
recent years, reflecting changing emotional themes in musical
preferences. These insights were crucial in effectively mapping
audio features to user moods, thereby refining recommendation

Rochester Institute of Technology

2|Page

RIT Computer Science • Capstone Report • 2245

accuracy.
B. Machine Learning Models
Facial emotion recognition was implemented using MobileNetV2 from tf.keras.applications, a lightweight CNN architecture specifically optimized for efficient real-time inference on mobile and embedded platforms. The architecture was
adapted and fine-tuned through transfer learning to recognize
three targeted emotional categories: Happy, Sad, and Neutral.
The resulting model comprises approximately 3.54 million parameters, with roughly 3.50 million parameters being trainable,
enabling the model to capture subtle facial cues effectively.
To enhance predictive accuracy and ensure robust feature
extraction, additional fully-connected layers with ReLU activations were integrated following the base MobileNetV2 architecture. The final classification was handled by a dense output
layer using softmax activation to predict the probability distribution across the emotional categories. The model employed
sparse categorical cross-entropy as the loss function, optimized
via the Adam optimizer, ensuring effective convergence and
efficient training.
For face detection within the emotion recognition pipeline,
the Haar Cascade frontal face detector was utilized. This
detector efficiently localized faces within captured images,
providing focused inputs to the CNN model, thereby improving classification accuracy and reducing inference latency. The
integrated approach achieved a high accuracy of approximately
91% on the FER-2013 test dataset, validating its practical
effectiveness for real-time applications.
For music recommendation, a combined machine learning
approach was adopted. A supervised multi-output Random
Forest classifier was used to predict the mood and era of songs
based on audio features such as loudness, energy, acousticness,
valence, danceability, tempo, and release year. This method
effectively captures intricate relationships between musical
attributes, though its accuracy strongly depends on the quality
and diversity of the labeled dataset. Figure 2 illustrates the
feature importance derived from the Random Forest model,
highlighting that ’loudness,’ ’energy,’ and ’acousticness’ are
the most influential attributes for accurate mood and era
predictions.
Additionally, an unsupervised method combining K-Means
clustering and K-Nearest Neighbors (KNN) was employed to
classify songs into distinct genres based on audio characteristics. This approach enables flexibility in handling unlabeled
data, although careful tuning of clustering parameters was
necessary to reflect user perceptions and genre distinctions
accurately. By integrating these supervised and unsupervised
methods, the system provided highly personalized and contextually appropriate music recommendations aligned closely
with user-detected emotional states and genre preferences.

Fig. 2. Random Forest Model Feature Importance

facial expression analysis, the system queried a preprocessed
and labeled music dataset to identify songs that matched both
the detected mood, as well as the user’s preferred genres
and eras. Songs were filtered based on predicted mood, genre
clusters, and relevant audio features, allowing the system to
generate recommendations tailored to the emotional context of
the user. The predicted mood and genre labels generated by
the machine learning models were stored in a relational SQL
database to support efficient querying and personalized filtering. This combined strategy ensured that recommendations remained both personalized and contextually relevant, enhancing
the emotional alignment and overall listening experience.
D. System Architecture
The system is structured into several interconnected components that work together to deliver emotion-aware music
recommendations. It follows a modular architecture to enable
smooth interaction between the user interface, data storage,
and machine learning models.
The frontend, developed using ReactJS, provides users with
an intuitive interface for capturing facial images, setting genre
preferences during onboarding, and viewing personalized music suggestions. It enables webcam access for image capture

C. Method
The recommendation process was designed to integrate realtime mood detection with users’ predefined genre preferences.
Following the classification of a user’s emotional state through
Rochester Institute of Technology

Fig. 3. System Architecture

3|Page

RIT Computer Science • Capstone Report • 2245

and displays real-time results from the emotion detection
module.
The backend, built with Flask, exposes RESTful APIs
that handle image uploads, trigger emotion classification, and
retrieve filtered music recommendations based on the user’s
current mood and preferences. It acts as the central orchestrator
for the system’s logic and data flow.
Facial emotion classification and music recommendation are
handled by integrated machine learning components in the
backend. The emotion detection pipeline begins with Haar
Cascade-based face localization. Detected faces are processed
using a lightweight convolutional network for mood recognition, and the resulting mood is used to query a recommendation engine that combines supervised and unsupervised models
to predict and filter songs based on mood, era, and genre
preferences. This pipeline ensures that music suggestions are
both personalized and emotionally relevant.
A relational SQL database stores user profiles, moodto-genre mappings, and recommendation history, supporting
stateful interactions and personalization. All system components—including the frontend, backend, and models—run
locally on a single machine, with public accessibility achieved
via Ngrok for testing and demonstration purposes.
Figure [3] presents a high-level view of the system architecture and the interactions between its core components.
IV. R ESULT
Surveys were conducted with a group of 30 participants,
each of whom completed two survey sessions, resulting in a
total of 60 responses. Participants were between 23 and 36
years old, representing a diverse but relevant demographic for
music recommendation systems. In each session, participants’
emotions were detected using the system, followed by music
recommendations based on their detected mood and genre
preferences. They then rated the accuracy of mood detection
and the quality of music suggestions in terms of relevance and
enjoyment.
The facial emotion recognition model achieved an accuracy
of approximately 91% on test data, demonstrating strong
performance in real-time emotion classification. Based on user

feedback, the system showed a perceived accuracy of 88%,
indicating close alignment between predicted and self-reported
moods.
Recommendations for the Happy mood received the highest satisfaction, with average scores for both relevance and
enjoyment exceeding 4.3 out of 5. Results for Excited and
Sad moods were also favorable, while slightly lower ratings for Neutral suggestions highlighted an area for future
improvement. Figure 4 illustrates the breakdown of average
recommendation ratings by mood, based on relevance and
enjoyment scores collected through user surveys. Overall, the
results support the effectiveness of combining emotion detection with genre-aware music filtering to deliver personalized,
emotionally aligned music recommendations.
V. C ONCLUSION
This research demonstrates that integrating real-time emotion detection with machine learning–based music recommendation can significantly enhance personalization in music
streaming systems. By combining facial expression analysis
with user-defined genre preferences, the system was able to
deliver recommendations that aligned closely with users’ emotional states. The emotion recognition model showed strong
performance in both test accuracy and user-perceived accuracy,
while survey feedback confirmed high levels of satisfaction
with the recommended music, particularly for moods like
Happy and Excited. Although results for the Neutral mood
were slightly less favorable, they indicate an opportunity for
further refinement. Overall, the system shows strong potential
for enabling emotionally intelligent music experiences and sets
a foundation for future work in adaptive, mood-aware content
delivery.
R EFERENCES
[1] Abdul, A., Chen, J., Liao, H.-Y., & Chang, S.-H. (2018). An EmotionAware Personalized Music Recommendation System Using a Convolutional Neural Networks Approach. Applied Sciences, 8(7), 1103. https:
//doi.org/10.3390/app8071103
[2] Ulleri, P., Prakash, S. H., Zenith, K. B., Nair, G. S., & Kannimoola, J. M.
(2021). Music recommendation system based on emotion. In 2021 12th
International Conference on Computing Communication and Networking
Technologies (ICCCNT), IIT-Kharagpur, India. https://doi.org/10.1109/
ICCCNT51525.2021.9579689
[3] De Prisco, R., Guarino, A., Malandrino, D., & Zaccagnino, R. (2022).
Induced Emotion-Based Music Recommendation through Reinforcement Learning. Applied Sciences, 12(21), 11209. https://www.mdpi.com/
2076-3417/12/21/11209
[4] FER-2013. https://www.kaggle.com/datasets/msambare/fer2013/data
[5] Malvani, V. (n.d.). Music Recommendation System Using
Spotify
Dataset.
https://www.kaggle.com/code/vatsalmavani/
music-recommendation-system-using-spotify-dataset/input
[6] Cristobalvch.
(n.d.).
Spotify
Machine
Learning.
GitHub.
https://github.com/cristobalvch/Spotify-Machine-Learning/blob/master/
data/data moods.csv

Fig. 4. Survey Response - Average Recommendation Rating by Mood

Rochester Institute of Technology

4|Page

