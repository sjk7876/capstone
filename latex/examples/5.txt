RIT Computer Science • Capstone Report • 20171

BoulderingDB: Annotation Tool and Dataset
Nathan Borkholder
Department of Computer Science
Golisano College of Computing and Information Sciences
Rochester Institute of Technology
Rochester, NY 14586
nrb4824@rit.edu

Abstract—We introduce a novel dataset designed to facilitate
bouldering analytics. This dataset consists of a diverse collection
of single-camera smartphone videos capturing climbing routes
of varying difficulty levels. Each video is annotated using a
custom-built annotation GUI, which leverages simple computer
vision algorithms to streamline the labeling process. We provide
a comprehensive overview of the annotation tool, detailing its
functionality and underlying logic. Additionally, we release a fully
annotated, publicly available dataset to support further research
and development. To assess the dataset’s effectiveness, we present
some summary statistics of the dataset including our thoughts
on the data distribution.
Index Terms—Climbing, Bouldering, Dataset, Video, Beta,
Route, Grade, Template Matching

I. I NTRODUCTION
Rock climbing, particularly bouldering, is a rapidly growing sport that combines cooperation with personal skill and
strength. It is common for climbers to work together on a
route to determine the best Beta (the intended sequence of
moves) or refine their technique. Bouldering is an iterative
process, where the primary goal is to improve upon previous
attempts. Bouldering is a form of rock climbing characterized
by shorter, more explosive routes where the climber is not
attached to a rope or harness. While each bouldering gym may
have slight variations in rules, the general principles remain
consistent. Climbers must begin with all limbs on designated
starting holds, which are typically marked with tape. In some
cases, only the hands are required to start on the designated
holds, while the feet may be placed on any hold within the
same route, on a volume, or directly on the wall. The route is
completed when the climber successfully reaches the final hold
or holds, also marked with tape, and demonstrates full control
of their body. This is often shown by hanging onto the final
hold for at least one second, preventing climbers from simply
jumping and making brief contact with it. At no point during
the climb can a participant touch the ground or use holds that
are not part of the designated route. In most cases, a route is
defined by holds of the same color from start to finish. For
example, if the starting holds are purple, the climber can only
use other purple holds to complete the climb. Additionally,
volumes — large hold structures that extend from the wall —
are often permitted for all routes and are typically marked in
a neutral color, such as gray, to distinguish them from routespecific holds. A single climbing attempt begins when the

climber lifts all limbs off the floor onto legal holds and ends
when they either maintain control of the final hold or fall off.
Climbers are always looking for ways to improve, and one
of the simplest and most effective tools is video. Recording
a session allows climbers to reflect on their technique, body
position, and overall performance. While some wearable devices have been developed to generate post-climb reports using
specialized hardware, they often lack the depth and context
that vision-based data can provide [1]. Without video, critical
aspects like movement quality and hold interaction are easily
missed.
This gap has sparked a growing demand for software tools
that leverage computer vision to analyze climbing footage.
Imagine an app that provides instant feedback on your climb,
or software that scans a wall and suggests optimized routes
tailored to your height, reach, and climbing style. The possibilities are truly endless. But to build these tools, the bouldering community needs high quality data: annotated climbing
footage that can power the next generation of climbing tech.
We introduce BoulderingDB, a new dataset designed to
address a major limitation of existing bouldering analytics
tools, the lack of a well-annotated dataset. Many tools must
create their own datasets, requiring significant time and effort,
which often results in small, mediocre datasets that lack
proper annotations. In cases where datasets are annotated, they
typically consist of still images of holds rather than videobased data. BoulderingDB resolves these issues by providing
a fully annotated dataset specifically designed for training
analytics models. Additionally, we introduce an annotation
tool that streamlines the process of dataset creation, enabling
users to generate custom datasets for unique use cases. This
tool incorporates computer vision techniques to automatically
track selected holds in videos and generate structured metadata
for easier model training. Both the dataset and annotation tool
are publicly available for the bouldering community to use in
future research and advancements in climbing analytics.
The remainder of this paper is structured as follows. Section
II describes the dataset and the collection process. Section
III presents the annotation tool’s design, functionality, and
underlying algorithms. Section IV includes analytics of the
dataset, including its key metrics and potential applications,
and analyzes the quality of the data. Section V reviews existing
datasets and models that have attempted to apply analytics to
bouldering. Finally, Section VI summarizes key contributions

Rochester Institute of Technology

1|Page

RIT Computer Science • Capstone Report • 20171

Fig. 1: Example Routes

and discusses future directions for improving both the dataset
and annotation tool.
The annotation tool1 and our initial 81-video dataset2 are
available publicly.
II. DATA S ET
The videos used for this study were collected from
Rochester Institute of Technology’s (RIT) Red Barn climbing
gym. The Red Barn was selected due to its familiarity to
the data collectors, which facilitated the annotation process.
Additionally, this location offered a diverse set of routes that
extended beyond a single flat wall, including underhangs,
overhangs, multi-wall configurations, and ceiling holds. These
more complex routes provide valuable data for training a
model, as simpler routes are easier to capture within a single
frame.
As this study involved human participants and collection of
video data, it required approval from the Rochester Institute of
Technology’s Institutional Review Board (IRB). The primary
ethical concerns addressed by the IRB included ensuring the
privacy of participants, minimizing potential risks associated
with data collection, and obtaining informed consent. Prior
to participating, all climbers were provided with a detailed
1 https://github.com/nrb4824/AnnotationTool2
2 https://github.com/nrb4824/BoulderingDBData

explanation of the study’s objectives, procedures, and any
potential risks. They were then given the opportunity to ask
questions and were required to sign an informed consent form
acknowledging their voluntary participation. To protect participants privacy no names or personal identifying information
are attached to any videos.
The Red Barn follows a color-coded system where all holds
for a given route share the same color. In cases where two
routes feature similar colors in close proximity, one of them
is marked with tape to distinguish it. The start holds are
identified by a taped-on sheet of paper displaying the route’s
name, grade, and directional arrows pointing to the starting
holds. The end of each route is marked with two pieces of
tape, often arranged in a shallow ‘V’ shape. Unless explicitly
stated otherwise, walls, ceilings, and volumes are assumed to
be allowed for all climbs.
The grading system at the Red Barn differs slightly from the
standard V-scale grading system. The following table provides
a general translation between the Red Barn’s grading system
and the traditional V-scale grading system:
BEG: V0-V1
NOV: V2-V3
• INT: V4-V5
• ADV: V6-V8
• EXP: V9+
•
•

Rochester Institute of Technology

2|Page

RIT Computer Science • Capstone Report • 20171

While slight variations exist for specific climbs, this classification serves as a general guideline.
Each video captures a single climbing attempt by one
climber. To ensure clear hold detection and tracking, climbers
were instructed to walk into the frame after recording began.
Climbers had the freedom to choose their routes and were
not required to complete them. The only requirement was
that they be able to start the route and complete at least two
moves (defined as moving a hand or foot to a different hold).
The dataset includes climbers of varying skill levels, multiple
attempts by the same climbers, unique routes, and unique betas
on each route as seen in Figure 1. As will be discussed later in
this paper, many of the recorded attempts were on lower-grade
routes. Each video is paired with a JSON file containing its
corresponding annotated data.
The data was collected using an iPhone 14 Pro. To accurately model the camera’s intrinsic parameters, we performed
offline calibration using the standard chessboard method. This
process yielded the following camera intrinsic matrix:

2803.91
 0
0

0
2804.78
0

(a) Bounding Box Annotation

(b) Video Playback


1518.38
2042.22
1
(c) Terminal Input

This matrix indicates that the camera has focal lengths of
approximately 2803.91 pixels in the x-direction and 2804.78
pixels in the y-direction, with the optical center located near
pixel coordinates (1518.38, 2042.22). It is important to note
that the videos used in the dataset are uncorrected, meaning
they retain the original lens distortion and camera perspective
captured during recording.
III. A NNOTATION T OOL
The Bouldering Video Annotation Tool streamlines the
process of analyzing bouldering videos by efficiently tracking
and annotating climbing holds using template matching. The
tool is implemented in Python and leverages OpenCV for
template matching and a simple graphical interface to enhance
usability.
A. Annotation Pipeline
The annotation pipeline transforms raw video input into
structured metadata. The general workflow consists of the
following steps (shown in Figure 2):
Bounding Box Annotation: Users draw bounding boxes
around all holds and specify the start and end holds.
• Video Playback Using the control panel, users can play
the video to mark when each hold is used, as well as
when the climber starts and finishes the climb. Video
playback features allow for easier navigation and precise
annotation.
• Terminal Input: The annotation tool provides prompts
in the terminal where it was executed, allowing users to
input supplementary details required for annotation.
•

Fig. 2: Annotation Pipeline. A) Red boxes are unused holds.
B) Yellow boxes are used holds. C) Green boxes are the start
and end holds. D) The blue box is the selected hold.

B. Output Format
The annotation tool generates a JSON file that is stored in a
predefined folder. This JSON file shares the same name as the
input video, ensuring easy correspondence between video files
and annotations. Additionally, the tool includes an optional
feature to save the annotated video frames as a new video.
While this feature is not necessary for annotation purposes,
it serves as a demonstration of the tool’s capabilities. An
example JSON output file is depicted in Figure 3 and detailed
below.
The JSON file stores general route based information:
• climber_id: Unique id per climber.
• climb_id: Unique id per climb.
• total_frames: Number of frames in the video.
• sample_interval: Frequency that hold information
is recorded.
• number_of holds: Number of holds in the route.
• color_of route: Color identified by annotator.
• route_rating: Grade or difficulty of the route.
• climb_valid: Indicates the outcome of the climb (e.g.,
Fell or Complete).
• start_frame: The frame the climb was started.
• end_frame: The frame the climb ended.
The JSON file also stores detailed hold-level information, sampled at regular frame intervals determined by the
sample_interval parameter:

Rochester Institute of Technology

3|Page

RIT Computer Science • Capstone Report • 20171

To improve efficiency, the search region for each template
is limited to an adjustable radius around its last detected
position. This optimization significantly reduces computational
overhead and enables multiple templates to be tracked simultaneously without compromising the tool’s performance.
Template matching proved effective within the context of
our human-in-the-loop annotation system. However, one limitation of template matching is its sensitivity to significant
changes in 3D perspective. While a human-in-the-loop system
can mitigate these issues, future automated systems should
consider more robust methods to detect and track holds.
IV. E VALUATION
A. Dataset Statistics

Fig. 3: Example JSON

Hold #: Automatically assigned hold ID during annotation.
• X: The x-coordinate of the center of the hold’s bounding
box.
• Y: The y-coordinate of the center of the hold’s bounding
box.
• Visible: A boolean indicating whether the hold is visible
(not occluded) in the frame.
• Start hold: A boolean flag indicating if the hold is a
designated start hold.
• End hold: A boolean flag indicating if the hold is a
designated end hold.
• Frame used: The frame number in which the hold is first
interacted with by the climber.
•

C. Template Matching for Hold Tracking
The bounding boxes drawn by the user serve as templates
for template matching, a process that identifies and tracks
holds across video frames. Template matching involves capturing a region of an image or video as a reference template
and then scanning the video to find similar patterns. Since
matches are rarely exact, a confidence threshold is applied.
For instance, a threshold of 80% ensures that the algorithm
detects regions with at least 80% similarity to the template.
This approach is intended to semi-automate the annotation
process, since manually tracking the (x, y) coordinates for
every hold across hundreds or thousands of frames would be
tedious and prone to error.

The BoulderingDB dataset comprises 81 annotated videos
ranging from 11 seconds to nearly two minutes. Although
the dataset is relatively small, it is larger than existing nonpublicly available datasets used to train analytical models,
which typically consist of only 30 to 50 unannotated videos.
In total, the dataset includes:
• 32 climbing participants.
• 49 different climbing routes.
• 50 completed climbs.
• 31 incomplete climbs.
On average, climbers used 76% of the total available holds
on a given route. We observed that climbers with lower skill
levels tended to use more holds compared to more experienced
climbers. It is likely that the 76% figure slightly underrepresents actual usage, as experienced climbers occasionally
climbed below their skill level, completing routes more quickly
and using fewer holds.
We also found that the average time for completed climbs
was 28 seconds, compared to 26 seconds for incomplete
climbs. Based on our observations during data collection, we
attribute the closeness of these durations to climbers often
becoming stuck on a specific section of the wall before falling.
In such cases, the climber would remain stationary until fatigue
set in, resulting in a fall. This period of stagnation contributed
significantly to the total climb duration.
B. Dataset Analytics
To better understand the dataset we present diagrams in
Figure 4. The Climbers Vs Climbs matrix illustrates which
climbs each participant attempted across the entire dataset.
This provides insight on how many times a climber attempted
a single climb. In a few cases participants would fail on an
attempt and would want to try again until they succeeded. One
climber even attempted the same climb three separate times
as they wanted to show they could complete the climb.
The Color of Route pie chart depicts the distribution of
hold colors used in the dataset. While this may not offer
deep analytical value on its own, it is particularly useful for
users interested in training models on the dataset. For instance,
automatic frame segmentation is a critical step in detecting
holds, and this chart highlights all the colors that a model
must recognize, as well as their relative frequencies. But the

Rochester Institute of Technology

4|Page

RIT Computer Science • Capstone Report • 20171

Fig. 4: Dataset Statistics

real challenge of color detection is that lighting and chalk play
a big role in how easy the color is to detect. In the table to the
right of the pie chart are holds that fit each color but either
look very different or the color is rather difficult to identify.
This dataset provides the true color value for each climb and
identifies all the holds within the climb, creating a benchmark
for other hold color detection algorithms.
The Time vs. Holds Used scatter plot demonstrates a
roughly linear relationship between the number of holds
used and the duration of a completed climb. This correlation
is significant, suggesting that the presence of experienced
climbers tackling easier routes did not substantially distort the
trend. Supporting this observation, the Number of Holds and
Unique Holds Used histograms reinforce the same relationship. On average, completed climbs featured 15 total holds,
with climbers utilizing approximately 12 to 13 of them.

Figure 5. The Route Ratings vs. Climbers matrix displays the
number of climbs attempted by each climber across different
grade brackets. Fortunately, the dataset includes a strong
representation of Novice and Intermediate climbers, along with
a few highly active Beginners who attempted many routes.
Although we were unable to directly ask participants about
their self-assessed skill levels due to IRB restrictions, the
resulting distribution still provides a reasonable approximation
of overall skill diversity in the dataset.

Fig. 6: Time of Climbs

Fig. 5: Distribution of Participants
We illustrate the distribution of participant skill levels in

We highlight the relationship between route grade and the
time required to complete each climb in Figure 6. The Time
of Climb vs. Route Rating box plot reveals a generally
linear trend, where higher-grade routes tend to take longer
to complete—an expected outcome in a bouldering dataset.
However, we believe the true trend may be slightly less

Rochester Institute of Technology

5|Page

RIT Computer Science • Capstone Report • 20171

steep than the plot suggests. This is due to more experienced
climbers occasionally choosing to attempt easier routes when
fatigued. These climbers were still able to complete the easier
routes much faster than climbers whose skill levels matched
the route difficulty, even when tired.

We believe BoulderingDB lays a strong foundation for
future advancements in automated climbing analysis. As future
work, we plan to expand the dataset with additional videos
from other climbing gyms, creating a more robust dataset. We
hope that others will aid in the expansion of this dataset and
add their own improvements to the annotation tool.

V. R ELATED W ORK

R EFERENCES

Pre-existing bouldering datasets are often limited in scope
and usability. Most publicly available datasets consist primarily of static images rather than videos [2], [3]. These images
capture various hold colors, shapes, and other useful features
for general analytics of bouldering holds [2], [3]. While such
datasets are valuable for generating tabular data on climbing
holds, they are not designed to track a climber’s movement
over time from video footage.
Several comprehensive datasets exist that provide in-depth
analysis of climbing-related data; However, the raw data is
rarely made publicly accessible [4]. To generate meaningful
Post-climb reports that offer actionable insights for climbers,
video-based datasets are essential. While some research groups
have attempted to develop such tools with varying degrees
of success, access to their datasets remains a significant
limitation.
For example, BoulderVision is a computer vision-based
model that detects climbing holds, estimates climber poses,
and tracks movement to create detailed post-climbing reports
[5]. However, while the tool itself is publicly available, the
dataset used for training the model is not, making it difficult
for researchers to replicate or improve upon the system.
Currently, there are very few easily accessible, video-based
bouldering datasets. Those that do exist are often embedded
within proprietary analytics tools, limiting their transparency
and usability.
Furthermore, the quality and completeness of these datasets
remain uncertain, as many were developed by student research
groups primarily focused on building analytics tools rather
than curating a robust dataset [1]. Additionally, these datasets
frequently rely on multiple cameras or external sensors to
streamline the segmentation and annotation process [6].

[1] S. Ekaireb, M. A. Khan, P. Pathuri, P. H. Bhatia, R. Sharma,
and N. Manjunath-Murkal. (2022) Computer vision based indoor
rock climbing analysis. [Online]. Available: https://kastner.ucsd.edu/ryan/
wp-content/uploads/sites/5/2022/06/admin/rock-climbing-coach.pdf
[2] Pwals. (2023, nov) Bouldering holds dataset. Visited on
2025-02-20. [Online]. Available: https://universe.roboflow.com/pwals/
bouldering-holds-9wavr
[3] Blackcreed. (2024, jun) Climbing holds and volumes dataset. Visited
on 2025-02-20. [Online]. Available: https://universe.roboflow.com/
blackcreed-xpgxh/climbing-holds-and-volumes
[4] A. Karthik. Climbdb. [Online]. Available: https://devpost.com/software/
climbdb
[5] D. Reiff. (2024). [Online]. Available: https://blog.roboflow.com/
bouldering
[6] I. Ivanova. (2022, Mar) Climbing activity recognition with
video data. [Online]. Available: https://medium.com/@yustinaivanova/
climbing-activity-recognition-using-video-data-684f68b42a42

VI. C ONCLUSION
In this paper, we introduced BoulderingDB, a novel, publicly available dataset designed to support and accelerate
research in vision-based climbing analytic tools. By collecting
and annotating 81 climbing videos with a custom-built annotation tool, we address the current lack of video based annotated
data for bouldering. This dataset captures diverse routes and
climbers of varying skill levels, providing much needed data
for potential machine learning and computer vision models.
The annotation tool was designed to be easy to use and streamline annotations through an intuitive interface and efficient
template-matching algorithm, enabling both researchers and
the broader bouldering community. We acknowledge that data
collection and annotation is a time consuming task, but it is
essential for research and development.
Rochester Institute of Technology

6|Page

